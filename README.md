# EXP-3-PROMPT-ENGINEERING-

## Aim: 
Evaluation of 2024 Prompting Tools Across Diverse AI Platforms: 
ChatGPT, Claude, Bard, Cohere Command, and Meta
Experiment:
Within a specific use case (e.g., summarizing text, answering technical questions), compare the performance, user experience, and response quality of prompting tools across these different AI platforms.

## Algorithm:
1. Define tasks & metrics (faithfulness, format adherence, citations, latency, cost).
2. Prepare standardized inputs & prompts (same article, same technical question).
3. Run each model with fixed settings; log outputs and metadata.
4. Score with a rubric; verify claims/citations; compute aggregate stats.
5. Synthesize findings into a structured report with summaries, templates, and recommendations.
   
## Prompt
Write a professional report comparing 2024 prompting tools across ChatGPT, Claude, Bard/Gemini, Cohere Command, and Meta (Llama). Focus on two use cases: summarizing a long technical article and answering a moderately hard technical question. Include methodology, evaluation criteria, key strengths/weaknesses, detailed comparisons, recommendations, and example prompt templates. Keep the tone analytical, structured with clear headings and bullet points where needed, and avoid marketing language.

## Output
[exp 3 prompt.pdf](https://github.com/user-attachments/files/22131054/exp.3.prompt.pdf)

## Result
Evaluating the comparison of 2024 prompting tools—ChatGPT, Claude, Bard, Cohere Command, and Meta—evaluates their performance, user experience, and response quality within a specific use case. Leading prompt engineering platforms like PromptLayer and PromptPerfect enhance these AIs’ capabilities by optimizing prompt creation, management, and output quality, ensuring effective AI-driven solutions.
